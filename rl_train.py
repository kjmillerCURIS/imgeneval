# -*- coding: utf-8 -*-
"""RL Train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QTo-RiHHcqAqsDAQyX9oP0bQdFtGefXc
"""

import json
import random
import requests
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from io import BytesIO
from collections import deque
from sentence_transformers import SentenceTransformer
from transformers import (
    CLIPProcessor,
    CLIPModel,
    BlipProcessor,
    BlipForQuestionAnswering,
    ViltProcessor,
    ViltForQuestionAnswering
)

# Device Configuration
device = "cuda" if torch.cuda.is_available() else "cpu"

# Helper Function for Image Loading
def load_image_from_url(image_url: str) -> Image.Image:
    """Load image from URL with error handling"""
    try:
        response = requests.get(image_url)
        response.raise_for_status()
        return Image.open(BytesIO(response.content))
    except Exception as e:
        print(f"Error loading image from {image_url}: {e}")
        return Image.new('RGB', (224, 224))  # Return blank image as fallback

# BLIP Implementation
processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
model_blip = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to(device)

def answer_with_blip(image: Image.Image, question: str) -> str:
    """Answer a VQA question using BLIP with a PIL Image"""
    try:
        inputs = processor_blip(image, question, return_tensors="pt").to(device)
        with torch.no_grad():
            output = model_blip.generate(**inputs)
        return processor_blip.decode(output[0], skip_special_tokens=True)
    except Exception as e:
        print(f"BLIP error: {e}")
        return "error"

# VILT Implementation
processor_vilt = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
model_vilt = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa").to(device)

def answer_with_vilt(image: Image.Image, question: str) -> str:
    """Answer a VQA question using VILT with a PIL Image"""
    try:
        encoding = processor_vilt(image, question, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model_vilt(**encoding)
        return model_vilt.config.id2label[outputs.logits.argmax(-1).item()]
    except Exception as e:
        print(f"VILT error: {e}")
        return "error"


def load_mscoco_samples(json_path: str = '/content/coco_image_data_final2_with_context&preds.json', train_percentage: float = 0.8):
    """
    Load the MSCOCO samples from a JSON file and split them into training and testing datasets.

    Args:
        json_path (str): Path to the JSON file.
        train_percentage (float): Fraction of the data to be used for training (between 0 and 1).

    Returns:
        tuple: (train_dataset, test_dataset)
    """
    try:
        with open(json_path, 'r') as file:
            data = json.load(file)

        # Determine the split index
        split_index = int(len(data) * train_percentage)
        train_data = data[:split_index]
        test_data = data[split_index:]

        return Dataset(train_data), Dataset(test_data)

    except FileNotFoundError:
        print(f"Error: JSON file not found at {json_path}")
        return Dataset([]), Dataset([])

# Example usage:
# train_ds, test_ds = load_mscoco_samples('/path/to/coco_data.json', train_percentage=0.75)


class Dataset:
    def __init__(self, data, batch_size=1):
        self.data = data
        self.batch_size = batch_size
        self.index = 0  # Iterator index

    def __iter__(self):
        self.index = 0
        return self

    def __next__(self):
        if self.index >= len(self.data):
            raise StopIteration
        batch = self.data[self.index:self.index + self.batch_size]
        self.index += self.batch_size
        return batch

    def __len__(self):
        """Returns the number of batches"""
        return (len(self.data) + self.batch_size - 1) // self.batch_size  # Round up

    def sample(self):
        """Returns a single random sample"""
        return random.choice(self.data) if self.data else None


class QNetwork(nn.Module):
    def __init__(self, input_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.fc(x)

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward):
        self.buffer.append((state, action, reward))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards = zip(*batch)
        return np.array(states), np.array(actions), np.array(rewards)

    def __len__(self):
        return len(self.buffer)

# Action Space Configuration
ACTION_SPACE = {
    0: {"name": "Blip", "function": answer_with_blip},
    1: {"name": "Vilt", "function": answer_with_vilt},
}

# Context Builder with proper image handling
class ContextBuilder:
    def __init__(self):
        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    def build(self, image: Image.Image, question: str) -> np.ndarray:
        """Build context vector from image and question"""
        try:
            # Text embedding
            text_embed = self.text_encoder.encode([question])[0]

            # Image embedding
            inputs = self.clip_processor(
                images=image,
                return_tensors="pt"
            )
            with torch.no_grad():
                image_embed = self.clip_model.get_image_features(**inputs)
                image_embed = image_embed / image_embed.norm(dim=-1, keepdim=True)

            return np.concatenate([text_embed, image_embed.cpu().numpy().flatten()])
        except Exception as e:
            print(f"Context building error: {e}")
            return np.zeros(384 + 512)  # Return zero vector on error

train,test = load_mscoco_samples()
print(len(train.data))
print(len(test.data))

"""# New Section"""

count = 0
for train_ins in train.data:
  for test_inst in test.data:

    if train_ins['coco_url'] == test_inst['coco_url']:
      count+=1
print(count)

"""# Training Part"""

import json
import random
import requests
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from io import BytesIO
from collections import deque
from sentence_transformers import SentenceTransformer
from transformers import (
    CLIPProcessor,
    CLIPModel,
    BlipProcessor,
    BlipForQuestionAnswering,
    ViltProcessor,
    ViltForQuestionAnswering
)

# Device Configuration
device = "cuda" if torch.cuda.is_available() else "cpu"

# Helper Function for Image Loading
def load_image_from_url(image_url: str) -> Image.Image:
    """Load image from URL with error handling"""
    try:
        response = requests.get(image_url)
        response.raise_for_status()
        return Image.open(BytesIO(response.content))
    except Exception as e:
        print(f"Error loading image from {image_url}: {e}")
        return Image.new('RGB', (224, 224))  # Return blank image as fallback

processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
model_blip = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to(device)

def answer_with_blip(image: Image.Image, question: str) -> str:
    """Answer a VQA question using BLIP with a PIL Image"""
    try:
        inputs = processor_blip(image, question, return_tensors="pt").to(device)
        with torch.no_grad():
            output = model_blip.generate(**inputs)
        return processor_blip.decode(output[0], skip_special_tokens=True)
    except Exception as e:
        print(f"BLIP error: {e}")
        return "error"

processor_vilt = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
model_vilt = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa").to(device)

def answer_with_vilt(image: Image.Image, question: str) -> str:
    """Answer a VQA question using VILT with a PIL Image"""
    try:
        encoding = processor_vilt(image, question, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model_vilt(**encoding)
        return model_vilt.config.id2label[outputs.logits.argmax(-1).item()]
    except Exception as e:
        print(f"VILT error: {e}")
        return "error"


def load_mscoco_samples(json_path: str = '/content/coco_image_data_final2_with_context&preds.json', train_percentage: float = 0.8):
    """
    Load the MSCOCO samples from a JSON file and split them into training and testing datasets.

    Args:
        json_path (str): Path to the JSON file.
        train_percentage (float): Fraction of the data to be used for training (between 0 and 1).

    Returns:
        tuple: (train_dataset, test_dataset)
    """
    try:
        with open(json_path, 'r') as file:
            data = json.load(file)

        # Determine the split index
        split_index = int(len(data) * train_percentage)
        train_data = data[:split_index]
        test_data = data[split_index:]

        return Dataset(train_data), Dataset(test_data)

    except FileNotFoundError:
        print(f"Error: JSON file not found at {json_path}")
        return Dataset([]), Dataset([])

# Example usage:
# train_ds, test_ds = load_mscoco_samples('/path/to/coco_data.json', train_percentage=0.75)


class Dataset:
    def __init__(self, data, batch_size=1):
        self.data = data
        self.batch_size = batch_size
        self.index = 0  # Iterator index

    def __iter__(self):
        self.index = 0
        return self

    def __next__(self):
        if self.index >= len(self.data):
            raise StopIteration
        batch = self.data[self.index:self.index + self.batch_size]
        self.index += self.batch_size
        return batch

    def __len__(self):
        """Returns the number of batches"""
        return (len(self.data) + self.batch_size - 1) // self.batch_size  # Round up

    def sample(self):
        """Returns a single random sample"""
        return random.choice(self.data) if self.data else None


class QNetwork(nn.Module):
    def __init__(self, input_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )

    def forward(self, x):
        return self.fc(x)

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward):
        self.buffer.append((state, action, reward))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards = zip(*batch)
        return np.array(states), np.array(actions), np.array(rewards)

    def __len__(self):
        return len(self.buffer)

# Action Space Configuration
ACTION_SPACE = {
    0: {"name": "Blip", "function": answer_with_blip},
    1: {"name": "Vilt", "function": answer_with_vilt},
}

# Context Builder with proper image handling
class ContextBuilder:
    def __init__(self):
        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    def build(self, image: Image.Image, question: str) -> np.ndarray:
        """Build context vector from image and question"""
        try:
            # Text embedding
            text_embed = self.text_encoder.encode([question])[0]

            # Image embedding
            inputs = self.clip_processor(
                images=image,
                return_tensors="pt"
            )
            with torch.no_grad():
                image_embed = self.clip_model.get_image_features(**inputs)
                image_embed = image_embed / image_embed.norm(dim=-1, keepdim=True)

            return np.concatenate([text_embed, image_embed.cpu().numpy().flatten()])
        except Exception as e:
            print(f"Context building error: {e}")
            return np.zeros(384 + 512)  # Return zero vector on error

def train():
    # Hyperparameters
    INPUT_DIM = 384 + 512
    ACTION_DIM = len(ACTION_SPACE)
    BUFFER_CAPACITY = 1000
    BATCH_SIZE = 32
    EPOCHS = 50
    EPSILON_START = 1.0
    EPSILON_END = 0.1

    # Initialize components
    q_net = QNetwork(INPUT_DIM, ACTION_DIM).to(device)
    optimizer = optim.Adam(q_net.parameters(), lr=0.001)
    buffer = ReplayBuffer(BUFFER_CAPACITY)
    context_builder = ContextBuilder()
    epsilon = EPSILON_START

    # Load dataset
    train,test = load_mscoco_samples()
    print(len(train.data))
    print(len(test.data))
    count = 0
    for train_ins in train.data:
      for test_inst in test.data:
        if train_ins['coco_url'] == test_inst['coco_url']:
          count+=1
    print('count: ',count)

    for epoch in range(EPOCHS):
        for batch in train:
            for sample in batch:
                img_url, questions = sample['coco_url'], sample['questions']

                for question_obj in questions:
                    question = question_obj['question']
                    ground_truth = question_obj['answer']

                    # Build context
                    context = question_obj['context']

                    if np.random.rand() < epsilon:
                        action = np.random.randint(ACTION_DIM)
                    else:
                        with torch.no_grad():
                            q_values = q_net(torch.FloatTensor(context).to(device))
                            action = torch.argmax(q_values).item()

                    # Check if both BLIP and VILT failed (rewards == -1) and override action
                    '''
                    if question_obj['blip_reward'] == -1 and question_obj['vilt_reward'] == -1:
                        action = 2
                    '''


                    # Get prediction
                    try:
                        reward = 0
                        if action == 0:
                            reward = question_obj['blip_reward']
                            predicted = question_obj['blip_pred']
                        elif action == 1:
                            reward = question_obj['vilt_reward']
                            predicted = question_obj['vilt_pred']

                        elif action == 2:
                            reward = 1
                            predicted = -2

                        #print(action)
                        #print(reward)
                    except KeyError:
                        print(f"Invalid action {action}")
                        reward = -1
                    except Exception as e:
                        print(f"Prediction error: {e}")
                        reward = -1

                    # Store in replay buffer
                    try:
                        buffer.add(context, action, reward)
                    except ValueError as e:
                        print(f"Buffer error: {e}")
                        continue

            if len(buffer) >= BATCH_SIZE:
                contexts, actions, rewards = buffer.sample(BATCH_SIZE)
                contexts = torch.FloatTensor(contexts).to(device)
                actions = torch.LongTensor(actions).to(device)
                rewards = torch.FloatTensor(rewards).to(device)
                #print(actions)
                #print(rewards)
                q_values = q_net(contexts)
                #print(q_values)
                q_selected = q_values.gather(1, actions.unsqueeze(1)).squeeze()
                #print(q_selected)
                loss = nn.MSELoss()(q_selected, rewards)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

        epsilon = max(EPSILON_END, epsilon * 0.995)
        print(f"Epsilon: {epsilon}")

        # Save model after each epoch
        if epoch % 10 == 0:
          model_save_path = f"trained_q_network_epoch_{epoch}.pth"
          torch.save({
              'model_state_dict': q_net.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'input_dim': INPUT_DIM,
              'action_dim': ACTION_DIM,
          }, model_save_path)
          print(f"Model saved to {model_save_path}")


train()

